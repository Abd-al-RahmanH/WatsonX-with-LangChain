{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c3e971eb-d7b5-4a9c-8859-3e6653c6f46d",
   "metadata": {},
   "source": [
    "# WatsonX with LangChain\n",
    "\n",
    "In this notebook we will review how to connect WatsonX with Langchain. We will read a text document and we will ask questions to this document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de9311a9-f30a-412f-a498-e3f18b8dfec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import Any, List, Mapping, Optional, Union, Dict\n",
    "from pydantic import BaseModel, Extra\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain, SimpleSequentialChain\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.llms.utils import enforce_stop_tokens\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n",
    "import os\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72646843-ac69-4131-b649-e3f43c58f2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "project_id = os.getenv(\"PROJECT_ID\", None)\n",
    "credentials = {\n",
    "    \"url\": \"https://us-south.ml.cloud.ibm.com\",\n",
    "    \"apikey\": os.getenv(\"API_KEY\", None)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd392fc2-8944-4707-8b70-077f42dd1c1b",
   "metadata": {},
   "source": [
    "## Foundation Models on watsonx.ai\n",
    "List available models\n",
    "All avaliable models are presented under ModelTypes class. For more information refer to documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "942b6849-23ef-40e6-aff5-c48a5684f386",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FLAN_T5_XXL', 'FLAN_UL2', 'MT0_XXL', 'GPT_NEOX', 'MPT_7B_INSTRUCT2', 'STARCODER', 'LLAMA_2_70B_CHAT', 'GRANITE_13B_INSTRUCT', 'GRANITE_13B_CHAT']\n"
     ]
    }
   ],
   "source": [
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes\n",
    "\n",
    "print([model.name for model in ModelTypes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9bf968e6-644f-4067-a257-a4c0d333b142",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = ModelTypes.LLAMA_2_70B_CHAT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e693541-aca4-4508-9162-aab78edab13a",
   "metadata": {},
   "source": [
    "Defining the model parameters\n",
    "You might need to adjust model parameters for different models or tasks, to do so please refer to documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5493bb8c-12a3-448a-bd8c-acccc55602d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb7f3295-8fc4-4be1-9440-ce4c9da23aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: \"greedy\",\n",
    "    GenParams.MAX_NEW_TOKENS: 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "591a1b38-59ca-4bd6-9690-ed997fc4da81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About to create bearer\n",
      "Bearer retrieved\n"
     ]
    }
   ],
   "source": [
    "#this cell should never fail, and will produce no output\n",
    "import requests\n",
    "\n",
    "def getBearer(apikey):\n",
    "    form = {'apikey': apikey, 'grant_type': \"urn:ibm:params:oauth:grant-type:apikey\"}\n",
    "    print(\"About to create bearer\")\n",
    "#    print(form)\n",
    "    response = requests.post(\"https://iam.cloud.ibm.com/oidc/token\", data = form)\n",
    "    if response.status_code != 200:\n",
    "        print(\"Bad response code retrieving token\")\n",
    "        raise Exception(\"Failed to get token, invalid status\")\n",
    "    json = response.json()\n",
    "    if not json:\n",
    "        print(\"Invalid/no JSON retrieving token\")\n",
    "        raise Exception(\"Failed to get token, invalid response\")\n",
    "    print(\"Bearer retrieved\")\n",
    "    return json.get(\"access_token\")\n",
    "\n",
    "credentials[\"token\"] = getBearer(credentials[\"apikey\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f1ee48b-0470-422f-9e2b-30a45217a3aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "# Initialize the Watsonx foundation model\n",
    "llama_model = Model(\n",
    "    model_id=model_id, \n",
    "    params=parameters, \n",
    "    credentials=credentials,\n",
    "    project_id=project_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ec2b0e-a4c3-46a0-8fb9-e26c9fab544a",
   "metadata": {},
   "source": [
    "## Model's details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22c5c11f-320e-4f93-b1df-1a1bfce72d40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Llama-2-70b-chat is an auto-regressive language model that uses an optimized transformer architecture.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_model.get_details()['short_description']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37120253-8a5a-4a89-ae1f-036901cc9bd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_sequence_length': 4096}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_model.get_details()['model_limits']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b970403d-16f0-4c23-8369-fd5c18b20d7d",
   "metadata": {},
   "source": [
    "## Generation AI by   using Llama-2-70b-chat model.\n",
    "\n",
    "It is generated text , according to provided instruction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "091c3f73-a905-46a4-b4e4-b03140b1637e",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = \"Using the directions below, answer in a maximum of  2 sentences. \"\n",
    "question = \"What is the capital of Italy\"\n",
    "prompt=\" \".join([instruction, question])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24cb85e4-0cfb-4ce0-94c5-63f27643d259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51d5d76-53de-4201-9332-78f33734b9c4",
   "metadata": {},
   "source": [
    "### - Generate_text method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9315c1dc-cb7b-4338-9a99-2eeadd036356",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\nThe capital of Italy is Rome (Italian: Roma). Rome is the largest city in Italy and is located in the central-western part of the country. It is known for its rich history, architecture, art, and culture, and is home to many famous landmarks such as the Colosseum, the Pantheon, and the Vatican City.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_model.generate_text(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb17e594-00b8-4368-b1d6-ea6200f20577",
   "metadata": {},
   "source": [
    "### - Generate method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ea7181ab-3201-4dd9-94e6-833c8435ef07",
   "metadata": {},
   "outputs": [],
   "source": [
    "result=llama_model.generate(prompt)['results'][0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5985787a-3d35-4c0b-a2fa-5389463559a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'?\\nThe capital of Italy is Rome. Rome is a city located in the central-western part of the country, and it has been the capital of Italy since 1865.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "091f381d-8811-445b-af6c-98fb80d51bd4",
   "metadata": {},
   "source": [
    "The strucure of this method has the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b8f1ec1a-2613-4da9-92c2-6bf1ba176a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "result={'model_id': 'meta-llama/llama-2-70b-chat',\n",
    " 'created_at': '2023-10-24T18:58:01.390Z',\n",
    " 'results': [{'generated_text': '?\\nThe capital of Italy is Rome (Italian: Roma). Rome is the largest city in Italy and is located in the central-western part of the country. It is known for its rich history, architecture, art, and culture, and is home to many famous landmarks such as the Colosseum, the Pantheon, and the Vatican City.',\n",
    "   'generated_token_count': 79,\n",
    "   'input_token_count': 7,\n",
    "   'stop_reason': 'eos_token'}],\n",
    " 'system': {'warnings': [{'message': 'This model is a Non-IBM Product governed by a third-party license that may impose use restrictions and other obligations. By using this model you agree to its terms as identified in the following URL. URL: https://dataplatform.cloud.ibm.com/docs/content/wsj/analyze-data/fm-models.html?context=wx',\n",
    "    'id': 'DisclaimerWarning'}]}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d29c4-c334-4d37-9301-17bb977ca210",
   "metadata": {},
   "source": [
    "# Integration with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "718385d0-8082-415e-b2e6-a20002ad6da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "from ibm_watson_machine_learning.foundation_models.utils.enums import ModelTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61fb0ec-affd-4707-a678-9f324481b6a5",
   "metadata": {},
   "source": [
    "First we load our Foundation  Model as for example Flan UL2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b0fe0b6-bb3c-4828-b5a0-a1be55b73979",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    GenParams.DECODING_METHOD: \"greedy\",\n",
    "    GenParams.MAX_NEW_TOKENS: 200\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f86fb703-855f-4ffa-ae0e-6ce0379ee7fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ibm_watson_machine_learning.foundation_models import Model\n",
    "# Initialize the Watsonx foundation model\n",
    "flan_ul2_model = Model(\n",
    "    model_id=ModelTypes.FLAN_UL2, \n",
    "    credentials=credentials,\n",
    "    project_id=project_id,\n",
    "    params=parameters\n",
    "    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b72eedf9-30ee-43f1-9be6-2b13bfca76d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"What color is the {flower}?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "071e7275-8b08-461a-9663-1162f98274a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_chain = LLMChain(llm=flan_ul2_model.to_langchain(), \n",
    "                     prompt=PromptTemplate.from_template(prompt_template))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96d6f65a-aa28-4922-8f67-35caab1a37aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'flower': 'sunflower', 'text': 'yellow'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain('sunflower')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cca3ea6-0c96-4f08-933b-315bdd1c84e8",
   "metadata": {},
   "source": [
    "# Remembering chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa6daa5-3621-4ca9-869d-e1f77872353e",
   "metadata": {},
   "source": [
    "The ConversationalRetrievalQA chain builds on RetrievalQAChain to provide a chat history component.\n",
    "\n",
    "It first combines the chat history (either explicitly passed in or retrieved from the provided memory) and the question into a standalone question, then looks up relevant documents from the retriever, and finally passes those documents and the question to a question-answering chain to return a response.\n",
    "\n",
    "To create one, you will need a retriever. In the below example, we will create one from a vector store, which can be created from embeddings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "532ce867-b82b-4efe-9f82-ec31656c2d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "557b3ca0-adb6-4cec-aadf-3e2bcdba7792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import TextLoader\n",
    "loader = TextLoader(\"example.txt\")\n",
    "document = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce779a4b-7feb-4416-a029-27272bab31ec",
   "metadata": {},
   "source": [
    "When we load the documents with TexLoader the datatype of this object is `langchain.schema.document.Document` wich is a list of parts of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fe83ed5a-1b4b-4bb6-bebd-407d8cc09cc2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain.schema.document.Document"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(document[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da005e0-6591-4001-b7f6-9c070ab6ba8e",
   "metadata": {},
   "source": [
    "Additionally if we want only the contents in string format we just called it by using the method `.page_content` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53cf99d5-8c11-46b3-a286-d04d1268c561",
   "metadata": {},
   "source": [
    "### Splitting the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a4a1c3f-ebe4-4c9b-be47-f1b875e72a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1143, which is longer than the specified 1000\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(separator=\"\\n\",\n",
    "                                      chunk_size=1000, \n",
    "                                      chunk_overlap=200)\n",
    "# Split text into chunks\n",
    "documents = text_splitter.split_documents(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1cba7beb-094f-41fb-9018-ec715a1fa539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(documents[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "032ee859-e9ca-4b83-a902-e611de5eb2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works.\\nAttention Is All You Need\\nAshish Vaswani Google Brain avaswani@google.com\\n \\nNoam Shazeer Google Brain noam@google.com\\n \\nNiki Parmar Google Research nikip@google.com\\nJakob Uszkoreit Google Research usz@google.com\\n Llion Jones Google Research llion@google.com\\n \\nAidan N. Gomez † University of Toronto aidan@cs.toronto.edu\\nukasz Kaiser\\nGoogle Brain\\nlukaszkaiser@google.com\\nIllia Polosukhin ‡\\nillia.polosukhin@gmail.com\\nAbstract'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db499ca4-a4e3-4f7f-b7f1-c2a5f796dfe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f50f3a-979b-4d40-917c-904ac0f14737",
   "metadata": {},
   "source": [
    "We have created 45 pieces of text, in the next part we are going to create the embeddings of this text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914c4c3e-0b59-476d-a813-82f9d0d8bde2",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "There are different ways to embedding our text,  in this demo we want to use\n",
    "Tensorflow embenddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "38205c63-2986-41b7-8fb1-1a2101071404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.embeddings import TensorflowHubEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8024cf46-0ed9-4f21-aeab-a5c04a492015",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://tfhub.dev/google/universal-sentence-encoder-multilingual/3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "58213696-1afb-4e5d-8801-38a7fd2709df",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings  = TensorflowHubEmbeddings(model_url=url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cad02144-baaa-4c04-90b9-1337203ceb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks=[content.page_content for content in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ff7d06d4-cea7-4399-9cc2-9eaea3058361",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e591a20b-68d7-49fa-bf09-78e53ea4c910",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_texts(texts=text_chunks, embedding=embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4317be05-97a9-4bd8-a685-5f93119ccbfd",
   "metadata": {},
   "source": [
    "We can now create a memory object, which is necessary to track the inputs/outputs and hold a conversation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "004bd66c-2bb2-4812-8ea9-790b9b825130",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\",\n",
    "                                  return_messages=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07770f30-1f7d-4449-ac9a-8f9049311cad",
   "metadata": {},
   "source": [
    "We now initialize the ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9c0a3954-e7ea-4833-8117-c82f08498cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ConversationalRetrievalChain.from_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8060dc4-0e4a-48e0-b077-aff899a2d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#llm=flan_ul2_model.to_langchain()\n",
    "llm=llama_model.to_langchain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a5758393-1655-4944-b6f5-0fa7cbd0b1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm=llm, \n",
    "                                           retriever=vectorstore.as_retriever(), \n",
    "                                           memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3a5903b1-651a-43a1-803f-26027e7c8c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the topic about\"\n",
    "result = qa({\"question\": query})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bd92f0a3-f05c-4dea-bbc3-85181bd27de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The topic is about the Transformer model in deep learning, specifically discussing its use of self-attention and multi-head attention mechanisms.'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e99a90b1-b15f-4584-8953-9bc6184a10ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='What is the topic about'), AIMessage(content=' The topic is about the Transformer model in deep learning, specifically discussing its use of self-attention and multi-head attention mechanisms.')]), return_messages=True, memory_key='chat_history')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37b4974-9dcc-4948-9aaa-a88e152f5e18",
   "metadata": {},
   "source": [
    "## Pass in chat history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d21d492-70e8-41f0-bf92-67cf6cddc871",
   "metadata": {},
   "source": [
    "In the above example, we used a Memory object to track chat history. We can also just pass it in explicitly. In order to do this, we need to initialize a chain without any memory object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7fd1df01-122b-409e-8dd2-8b7d2fc18926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(ConversationalRetrievalChain.from_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "420ff4e7-9f64-45f2-82ca-fd72519d9467",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm=llama_model.to_langchain(),\n",
    "                                           retriever=vectorstore.as_retriever())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2093323e-ddb8-42a8-87ed-d80a0c583a64",
   "metadata": {},
   "source": [
    "Here's an example of asking a question with no chat history with Watsonx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "15ddf9cc-8e90-4322-be7a-c4ca455b5ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = []\n",
    "query = \"What is the topic  about\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "aee8a3a4-c28b-44a1-a6e3-6c4e013ae92c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The topic is about the Transformer model in natural language processing, specifically discussing the use of self-attention and multi-head attention in the model.'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e7cb80-81c3-4a01-80fa-29a3e9f817f2",
   "metadata": {},
   "source": [
    "Here's an example of asking a question with some chat history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "813bc811-0b6f-4d21-9a44-ce30cc3b5c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"What is Transformer\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8d8790a1-1d1f-4f82-9a71-5d18b11c8b33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe Transformer model is a sequence transduction model that relies entirely on self-attention mechanisms, dispensing with recurrence and convolutions. It is based on attention mechanisms and is used for various tasks such as machine translation, English constituency parsing, and text summarization. The Transformer model is the first transduction model that uses self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolutions. It has been shown to be superior in quality, more parallelizable, and requiring significantly less time to train than other sequence transduction models.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bf327f65-3ee6-496a-87eb-dbd1c8835fe0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('What is the topic  about',\n",
       "  ' The topic is about the Transformer model in natural language processing, specifically discussing the use of self-attention and multi-head attention in the model.')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc2d53-8b30-490f-b060-bc693c854ca1",
   "metadata": {},
   "source": [
    "**Congratulations!** You have finished this tutorial of WatsonX with LangChain.\n",
    "\n",
    "For more tutorials like this visit [https://ruslanmv.com/](https://ruslanmv.com/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (WatsonX)",
   "language": "python",
   "name": "watsonx"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
